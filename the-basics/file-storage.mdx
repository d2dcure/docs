---
title: "File Storage"
description: "This page explains how D2D Cure handles file storage, including the AWS S3 integration, supported file types, and how files are managed throughout the application. Learn about file uploads, downloads, and the security measures in place. ."
---


## AWS S3

### What is S3? 

Amazon S3, or Amazon Simple Storage Service, is **a cloud-based object storage service offered by Amazon Web Services (AWS)**. It provides a way to store and retrieve any amount of data, from anywhere, making it suitable for a wide range of applications. S3 is designed for durability, scalability, and security, with features like automatic encryption and access management tools.Â Objects are organized into buckets, which are containers for your data. 

### What are buckets? 

An Amazon S3 bucket is **a fundamental component of Amazon's Simple Storage Service (S3), acting as a container for storing and managing data in the cloud**. Essentially, it's a public cloud storage resource where data is stored as "objects" instead of traditional files. These buckets are highly scalable, durable, and secure, making them suitable for various storage needs, including backups, website hosting, and data archiving. For this codebase, the name of the bucket where everything is stored is `d2dcurebucketprod` . This is further split into 

### Interacting with S3

The configuration file `s3config.js` in the root directory is what establishes the connection between the codebase and the D2DCure S3 bucket in the cloud. As long as you have the proper environment variables configured the connection should establish automatically. Now when you want to actually interact with S3 (uploading, getting, or deleting files from the bucket), you would use the `s3.ts` API endpoint file: 

typescript s3.ts

// pages/api/s3.ts

import type { NextApiRequest, NextApiResponse } from 'next';

import s3 from '../../../s3config';

const BUCKET_NAME = 'd2dcurebucketprod';

// If uploading very large files, you can increase the bodyParser size limit

export const config = {

  api: {

    bodyParser: {

      sizeLimit: '25mb',

    },

  },

};

export default async function s3Handler(req: NextApiRequest, res: NextApiResponse) {

  try {

    const folder = (req.query.folder as string) || '';

    switch (req.method) {

      case 'GET': {

        // 1) If ?download=\<filename\>, we generate a presigned URL for that specific file

        const downloadFileName = [req.query.download](http://req.query.download) as string;

        if (downloadFileName) {

          const Key = folder ? `${folder}/${downloadFileName}` : downloadFileName;

          const url = await s3.getSignedUrlPromise('getObject', {

            Bucket: BUCKET_NAME,

            Key,

            Expires: 60 // URL valid for 60 seconds

          });

          return res.status(200).json({ presignedUrl: url, fileKey: Key });

        }

        // 2) Otherwise, we list all objects under the folder prefix

        const listParams = {

          Bucket: BUCKET_NAME,

          Prefix: folder // e.g. "gel-images/", "kinetic_assays/raw", etc.

        };

        const data = await s3.listObjectsV2(listParams).promise();

        return res.status(200).json({

          objects:

            data.Contents?.map((obj) =\> ({

              key: obj.Key,

              url: `https://${BUCKET_NAME}.s3.amazonaws.com/${obj.Key}`

            })) || []

        });

      }

      case 'POST': {

        // UPLOAD a new file as base64

        // we expect { newFileName, fileBase64 }

        const { newFileName, fileBase64 } = req.body;

        if (\!newFileName || \!fileBase64) {

          return res.status(400).json({ error: 'newFileName and fileBase64 are required' });

        }

        // Convert base64 to a Buffer

        const fileBuffer = Buffer.from(fileBase64, 'base64');

        // Construct the S3 key

        const Key = folder ? `${folder}/${newFileName}` : newFileName;

        const uploadParams = {

          Bucket: BUCKET_NAME,

          Key,

          Body: fileBuffer

          // ContentType: 'image/png' or 'text/csv', etc., if you know the type

        };

        await s3.upload(uploadParams).promise();

        const url = `https://${BUCKET_NAME}.s3.amazonaws.com/${Key}`;

        return res.status(200).json({

          message: 'Upload success',

          objectKey: Key,

          url

        });

      }

      case 'DELETE': {

        // DELETE an object from S3

        // We'll expect { key } in the request body

        const { key } = req.body || {};

        if (\!key) {

          return res.status(400).json({ error: 'Missing "key" in request body' });

        }

        const deleteParams = {

          Bucket: BUCKET_NAME,

          Key: key

        };

        await s3.deleteObject(deleteParams).promise();

        return res.status(200).json({

          message: 'Delete success',

          deletedKey: key

        });

      }

      default:

        return res.status(405).json({ error: 'Method not allowed' });

    }

  } catch (error: any) {

    console.error('S3 API Error:', error);

    return res.status(500).json({

      error: error.message || 'Unexpected error'

    });

  }

}

\`\`\`

**Features** of s3.ts: 

- **Base64 encoding** for file transfer

- **Automatic folder organization** based on file type

- **25MB file size limit** (configurable)

- **Unique file naming** to prevent conflicts

## File Management Flow

### Upload Process

1.  **User selects file** in the application

2. **File is converted** to base64 format

3. **API endpoint receives** the file data

4. **S3 upload** with proper folder organization

5. **Database record** updated with file reference

6. **Success confirmation** returned to user

### Download Process

1. **User requests file** download

2. **Application generates** presigned URL

3. **User receives** secure download link

4. **File downloads** directly from S3

5. **URL expires** after 60 seconds

### File Organization

1. **Automatic categorization** based on file type

2. **Folder structure** maintains organization

3. **Naming conventions** ensure consistency

4. **Metadata tracking** for file management

## Performance and Scalability

### File Size Limits

- **25MB maximum** per file upload

- **Configurable limits** in API configuration

- **Chunked uploads** for larger files (if needed)

### Storage Optimization

- **Efficient file formats** for different data types

- **Compression** for large datasets

- **CDN integration** for faster access (if configured)

### Backup and Recovery

- **S3 versioning** for file history

- **Cross-region replication** for disaster recovery

- **Automated backups** for data protection

## Error Handling

### Common Issues

- **File size exceeded**: Returns 400 error with size limit information

- **Invalid file format**: Validates file types before upload

- **Network errors**: Retry mechanisms for failed uploads

- **Permission errors**: Proper error messages for access issues

### Error Responses

`\`\\\`typescript

catch (error: any) {

  console.error('S3 API Error:', error);

  return res.status(500).json({

    error: error.message || 'Unexpected error'

  });

}

`\`\\\`

## Best Practices

### For Developers

- **Always validate** file types and sizes

- **Use appropriate** folder structures

- **Handle errors** gracefully

- **Monitor storage** usage and costs

### For Users

- **Use descriptive** file names

- **Organize files** in appropriate folders

- **Keep file sizes** reasonable

- **Backup important** data locally

### Security Guidelines

- **Never expose** AWS credentials in client code

- **Use presigned URLs** for file access

- **Implement proper** access controls

- **Monitor access** logs regularly

## Monitoring and Maintenance

### Storage Monitoring

- **Track storage usage** and costs

- **Monitor access patterns** for optimization

- **Set up alerts** for unusual activity

### File Lifecycle

- **Archive old files** automatically

- **Delete unused files** to reduce costs

- **Maintain file organization** for efficiency

### Performance Optimization

- **Use appropriate** file formats

- **Implement caching** for frequently accessed files

- **Optimize upload/download** speeds